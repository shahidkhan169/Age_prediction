{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk8_2bKrcaUD"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "def extract_tar_gz_files(tar_files, extract_to_dir):\n",
        "    if not os.path.exists(extract_to_dir):\n",
        "        os.makedirs(extract_to_dir)\n",
        "\n",
        "    for tar_file in tar_files:\n",
        "        with tarfile.open(tar_file, 'r:gz') as tar_ref:\n",
        "            tar_ref.extractall(extract_to_dir)\n",
        "\n",
        "tar_files = ['/content/drive/MyDrive/CycleGAN_Facial_Aging/part1.tar.gz', '/content/drive/MyDrive/CycleGAN_Facial_Aging/part2.tar.gz', '/content/drive/MyDrive/CycleGAN_Facial_Aging/part3.tar.gz']\n",
        "extract_to_dir = 'extracted_data'\n",
        "extract_tar_gz_files(tar_files, extract_to_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def combine_extracted_data(source_dirs, target_dir):\n",
        "    if not os.path.exists(target_dir):\n",
        "        os.makedirs(target_dir)\n",
        "\n",
        "    for source_dir in source_dirs:\n",
        "        for filename in os.listdir(source_dir):\n",
        "            if filename.endswith('.jpg'):\n",
        "                src_path = os.path.join(source_dir, filename)\n",
        "                dst_path = os.path.join(target_dir, filename)\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "source_dirs = ['extracted_data/part1', 'extracted_data/part2', 'extracted_data/part3']\n",
        "target_dir = 'combined_dataset'\n",
        "combine_extracted_data(source_dirs, target_dir)\n"
      ],
      "metadata": {
        "id": "Fc42OrzfdjZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kcTeN0NeDN6",
        "outputId": "b7bf583e-b52e-4c70-bd0c-95a5782b20b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Custom generator to load images and extract age from filenames\n",
        "class UTKFaceDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_filenames, batch_size, img_size=(128, 128), shuffle=True):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.indexes = np.arange(len(self.image_filenames))\n",
        "        self.shuffle = shuffle\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_filenames = [self.image_filenames[k] for k in batch_indexes]\n",
        "        X, y = self.__data_generation(batch_filenames)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, batch_filenames):\n",
        "        X = np.empty((len(batch_filenames), *self.img_size, 3))\n",
        "        y = np.empty((len(batch_filenames),))\n",
        "\n",
        "        for i, filename in enumerate(batch_filenames):\n",
        "            parts = filename.split('_')\n",
        "            age = int(parts[0])\n",
        "\n",
        "            img_path = os.path.join(data_dir, filename)\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img = img.resize(self.img_size)\n",
        "            img_array = np.array(img) / 255.0  # Normalize image\n",
        "\n",
        "            X[i] = img_array\n",
        "            y[i] = (age - age_min) / (age_max - age_min)  # Normalize age\n",
        "\n",
        "        return X, y\n",
        "\n",
        "# Data directory and image filenames\n",
        "data_dir = '/content/combined_dataset'\n",
        "image_filenames = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# Extract age range to normalize later\n",
        "ages = [int(f.split('_')[0]) for f in image_filenames]\n",
        "age_min = min(ages)\n",
        "age_max = max(ages)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_filenames, test_filenames = train_test_split(image_filenames, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create data generators for training and validation\n",
        "batch_size = 32\n",
        "train_generator = UTKFaceDataGenerator(train_filenames, batch_size, img_size=(128, 128), shuffle=True)\n",
        "test_generator = UTKFaceDataGenerator(test_filenames, batch_size, img_size=(128, 128), shuffle=False)\n",
        "\n",
        "# Model definition with reduced complexity\n",
        "def build_age_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape),  # Reduced filters\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),  # Reduced dense layer size\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='linear')  # Age prediction (regression)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Pruning parameters\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                             final_sparsity=0.90,\n",
        "                                                             begin_step=0,\n",
        "                                                             end_step=1000)\n",
        "}\n",
        "\n",
        "# Build and prune the model\n",
        "pruned_model = prune_low_magnitude(build_age_model((128, 128, 3)), **pruning_params)\n",
        "pruned_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Pruning callbacks\n",
        "callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
        "\n",
        "# Train the pruned model\n",
        "pruned_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=test_generator,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Strip pruning and save the pruned model\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "model_for_export.save('pruned_age_model.h5')\n",
        "\n",
        "# Convert the model to TensorFlow Lite with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the smaller TFLite model\n",
        "with open('age_prediction_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF9x_pQcdn16",
        "outputId": "b7cb54ad-cc5e-4b8d-bf05-9fce7a9bd4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "603/603 [==============================] - 310s 482ms/step - loss: 0.0878 - mae: 0.1607 - val_loss: 0.0290 - val_mae: 0.1278\n",
            "Epoch 2/20\n",
            "603/603 [==============================] - 249s 413ms/step - loss: 0.0327 - mae: 0.1391 - val_loss: 0.0282 - val_mae: 0.1261\n",
            "Epoch 3/20\n",
            "603/603 [==============================] - 246s 407ms/step - loss: 0.0291 - mae: 0.1300 - val_loss: 0.0278 - val_mae: 0.1241\n",
            "Epoch 4/20\n",
            "603/603 [==============================] - 282s 468ms/step - loss: 0.0278 - mae: 0.1268 - val_loss: 0.0272 - val_mae: 0.1233\n",
            "Epoch 5/20\n",
            "603/603 [==============================] - 248s 411ms/step - loss: 0.0272 - mae: 0.1252 - val_loss: 0.0262 - val_mae: 0.1231\n",
            "Epoch 6/20\n",
            "603/603 [==============================] - 277s 459ms/step - loss: 0.0269 - mae: 0.1242 - val_loss: 0.0262 - val_mae: 0.1251\n",
            "Epoch 7/20\n",
            "603/603 [==============================] - 244s 405ms/step - loss: 0.0268 - mae: 0.1239 - val_loss: 0.0260 - val_mae: 0.1217\n",
            "Epoch 8/20\n",
            "603/603 [==============================] - 245s 407ms/step - loss: 0.0261 - mae: 0.1225 - val_loss: 0.0253 - val_mae: 0.1226\n",
            "Epoch 9/20\n",
            "603/603 [==============================] - 244s 405ms/step - loss: 0.0259 - mae: 0.1219 - val_loss: 0.0253 - val_mae: 0.1212\n",
            "Epoch 10/20\n",
            "603/603 [==============================] - 278s 462ms/step - loss: 0.0259 - mae: 0.1219 - val_loss: 0.0245 - val_mae: 0.1175\n",
            "Epoch 11/20\n",
            "603/603 [==============================] - 242s 401ms/step - loss: 0.0255 - mae: 0.1207 - val_loss: 0.0254 - val_mae: 0.1222\n",
            "Epoch 12/20\n",
            "603/603 [==============================] - 243s 403ms/step - loss: 0.0252 - mae: 0.1200 - val_loss: 0.0245 - val_mae: 0.1180\n",
            "Epoch 13/20\n",
            "603/603 [==============================] - 282s 468ms/step - loss: 0.0253 - mae: 0.1201 - val_loss: 0.0247 - val_mae: 0.1176\n",
            "Epoch 14/20\n",
            "603/603 [==============================] - 243s 403ms/step - loss: 0.0250 - mae: 0.1192 - val_loss: 0.0242 - val_mae: 0.1191\n",
            "Epoch 15/20\n",
            "603/603 [==============================] - 279s 464ms/step - loss: 0.0250 - mae: 0.1196 - val_loss: 0.0244 - val_mae: 0.1179\n",
            "Epoch 16/20\n",
            "603/603 [==============================] - 244s 405ms/step - loss: 0.0248 - mae: 0.1190 - val_loss: 0.0238 - val_mae: 0.1164\n",
            "Epoch 17/20\n",
            "603/603 [==============================] - 250s 415ms/step - loss: 0.0247 - mae: 0.1187 - val_loss: 0.0243 - val_mae: 0.1194\n",
            "Epoch 18/20\n",
            "603/603 [==============================] - 279s 463ms/step - loss: 0.0245 - mae: 0.1184 - val_loss: 0.0242 - val_mae: 0.1158\n",
            "Epoch 19/20\n",
            "603/603 [==============================] - 242s 401ms/step - loss: 0.0245 - mae: 0.1180 - val_loss: 0.0252 - val_mae: 0.1219\n",
            "Epoch 20/20\n",
            "603/603 [==============================] - 245s 407ms/step - loss: 0.0245 - mae: 0.1184 - val_loss: 0.0237 - val_mae: 0.1148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbz4y7dceH-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}